models:
  - hf_repo_id: "BAAI/bge-m3"
    handler: "app.models.hf_embedding.HFEmbeddingModel"
    supports_structured_outputs: false
  - hf_repo_id: "google/embeddinggemma-300m"
    handler: "app.models.hf_embedding.HFEmbeddingModel"
    supports_structured_outputs: false
  - hf_repo_id: "Qwen/Qwen3-VL-4B-Instruct-FP8"
    handler: "app.models.qwen_vl.QwenVLChat"
    supports_structured_outputs: false
    defaults:
      temperature: 0.7
      top_p: 0.9
      max_tokens: 512
  - hf_repo_id: "Qwen/Qwen3-VL-2B-Instruct-FP8"
    handler: "app.models.qwen_vl.QwenVLChat"
    supports_structured_outputs: false
    defaults:
      temperature: 0.7
      top_p: 0.9
      max_tokens: 512
  - hf_repo_id: "Qwen/Qwen3-VL-4B-Instruct"
    handler: "app.models.qwen_vl.QwenVLChat"
    supports_structured_outputs: false
    defaults:
      temperature: 0.7
      top_p: 0.9
      max_tokens: 512
  - hf_repo_id: "Qwen/Qwen3-VL-2B-Instruct"
    handler: "app.models.qwen_vl.QwenVLChat"
    supports_structured_outputs: false
    defaults:
      temperature: 0.7
      top_p: 0.9
      max_tokens: 512
  - hf_repo_id: "Qwen/Qwen3-4B-Instruct-2507"
    handler: "app.models.text_chat.TextChatModel"
    supports_structured_outputs: false
    defaults:
      temperature: 0.7
      top_p: 0.9
      max_tokens: 512
  - hf_repo_id: "Qwen/Qwen3-4B-Instruct-2507-FP8"
    handler: "app.models.text_chat.TextChatModel"
    supports_structured_outputs: false
    defaults:
      temperature: 0.7
      top_p: 0.9
      max_tokens: 512
  - hf_repo_id: "meta-llama/Llama-3.2-1B-Instruct"
    handler: "app.models.text_chat.TextChatModel"
    supports_structured_outputs: false
    defaults:
      temperature: 0.7
      top_p: 0.9
      max_tokens: 512
  - hf_repo_id: "meta-llama/Llama-3.2-3B-Instruct"
    handler: "app.models.text_chat.TextChatModel"
    supports_structured_outputs: false
    defaults:
      temperature: 0.7
      top_p: 0.9
      max_tokens: 512
  # Whisper ASR/translation models
  - hf_repo_id: "jethrowang/whisper-tiny-chinese"
    handler: "app.models.whisper.WhisperASR"
    supports_structured_outputs: false
  - hf_repo_id: "Ivydata/whisper-small-japanese"
    handler: "app.models.whisper.WhisperASR"
    supports_structured_outputs: false
  - hf_repo_id: "BELLE-2/Belle-whisper-large-v3-zh"
    handler: "app.models.whisper.WhisperASR"
    supports_structured_outputs: false
  - hf_repo_id: "whisper-large-v3-japanese-4k-steps"
    handler: "app.models.whisper.WhisperASR"
    supports_structured_outputs: false
  - hf_repo_id: "openai/whisper-large-v2"
    handler: "app.models.whisper.WhisperASR"
    supports_structured_outputs: false
  - hf_repo_id: "openai/whisper-medium"
    handler: "app.models.whisper.WhisperASR"
    supports_structured_outputs: false
  - hf_repo_id: "openai/whisper-medium.en"
    handler: "app.models.whisper.WhisperASR"
    supports_structured_outputs: false
  - hf_repo_id: "openai/whisper-small"
    handler: "app.models.whisper.WhisperASR"
    supports_structured_outputs: false
  - hf_repo_id: "openai/whisper-small.en"
    handler: "app.models.whisper.WhisperASR"
    supports_structured_outputs: false
  - hf_repo_id: "openai/whisper-tiny"
    handler: "app.models.whisper.WhisperASR"
    supports_structured_outputs: false
  - hf_repo_id: "openai/whisper-tiny.en"
    handler: "app.models.whisper.WhisperASR"
    supports_structured_outputs: false
