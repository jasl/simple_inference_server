# Example environment configuration for Simple Inference Server.
#
# Usage:
#   cp env.example .env
#   # edit .env, then start the service
#
# Notes:
# - `.env` is loaded automatically by Pydantic settings (see `app/config.py`).
# - `.env` is gitignored; this `env.example` is safe to commit.
# - Most settings have defaults, but `MODELS` is REQUIRED for the server to start.

# -----------------------------------------------------------------------------
# Required: select which models to load (comma-separated).
# Values must match `name` (preferred) or `hf_repo_id` entries from
# `configs/model_config.yaml` and optional `configs/model_config.local.yaml`.
# -----------------------------------------------------------------------------
MODELS=BAAI/bge-m3,Qwen/Qwen3-4B-Instruct-2507

# -----------------------------------------------------------------------------
# Model loading
# -----------------------------------------------------------------------------
MODEL_CONFIG_PATH=configs/model_config.yaml
MODEL_DEVICE=auto  # cpu | mps | cuda | cuda:<idx> | auto
AUTO_DOWNLOAD_MODELS=1
HF_HOME=./models

# -----------------------------------------------------------------------------
# Global concurrency & backpressure
# -----------------------------------------------------------------------------
MAX_CONCURRENT=2
MAX_QUEUE_SIZE=64
QUEUE_TIMEOUT_SEC=2

# Per-capability limiter overrides (optional; fall back to global values)
# EMBEDDING_MAX_CONCURRENT=
# CHAT_MAX_CONCURRENT=
# VISION_MAX_CONCURRENT=
# AUDIO_MAX_CONCURRENT=

# -----------------------------------------------------------------------------
# Thread pool sizing (does not bypass limiters)
# -----------------------------------------------------------------------------
EMBEDDING_MAX_WORKERS=2
EMBEDDING_COUNT_MAX_WORKERS=1
CHAT_MAX_WORKERS=2
CHAT_COUNT_MAX_WORKERS=1
VISION_MAX_WORKERS=1
AUDIO_MAX_WORKERS=1

# -----------------------------------------------------------------------------
# Batching
# -----------------------------------------------------------------------------
ENABLE_EMBEDDING_BATCHING=1
EMBEDDING_BATCH_WINDOW_MS=6
EMBEDDING_BATCH_WINDOW_MAX_SIZE=16
EMBEDDING_BATCH_QUEUE_SIZE=64
EMBEDDING_BATCH_QUEUE_TIMEOUT_SEC=2

ENABLE_CHAT_BATCHING=1
CHAT_BATCH_WINDOW_MS=10
CHAT_BATCH_MAX_SIZE=8
CHAT_BATCH_QUEUE_SIZE=64
CHAT_BATCH_ALLOW_VISION=0

# -----------------------------------------------------------------------------
# Request limits / timeouts
# -----------------------------------------------------------------------------
MAX_BATCH_SIZE=32
MAX_TEXT_CHARS=20000
MAX_AUDIO_BYTES=26214400

EMBEDDING_GENERATE_TIMEOUT_SEC=60
CHAT_GENERATE_TIMEOUT_SEC=60
AUDIO_PROCESS_TIMEOUT_SEC=180

# -----------------------------------------------------------------------------
# Warmup (startup guardrail)
# -----------------------------------------------------------------------------
ENABLE_WARMUP=1
WARMUP_BATCH_SIZE=1
WARMUP_STEPS=1
WARMUP_INFERENCE_MODE=1
# WARMUP_ALLOWLIST=
# WARMUP_SKIPLIST=

# -----------------------------------------------------------------------------
# Observability
# -----------------------------------------------------------------------------
ENABLE_METRICS=1
LOG_LEVEL=INFO

# -----------------------------------------------------------------------------
# Security
# -----------------------------------------------------------------------------
# TRUST_REMOTE_CODE_ALLOWLIST=Qwen/Qwen3-VL-4B-Instruct,Qwen/Qwen3-VL-2B-Instruct
TRUST_REMOTE_CODE_ALLOWLIST=

# Remote image fetching for vision models (Qwen3-VL) is disabled by default.
ALLOW_REMOTE_IMAGES=0
REMOTE_IMAGE_TIMEOUT=5
MAX_REMOTE_IMAGE_BYTES=5242880
REMOTE_IMAGE_HOST_ALLOWLIST=

